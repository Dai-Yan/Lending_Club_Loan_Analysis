{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Loan Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we use supervised learning models to predict the loan will be defaulted or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Part 1: Load Data](#Part-1:-Load-Data)\n",
    "* [Part 2: Data Preprocess](#Part-2:-Data-Preprocess)\n",
    "* [Part 3: Data Cleaning](#Part-3:-Data-Cleaning)\n",
    "* [Part 4: Data Visualization](#Part-4:-Data-Visualization)\n",
    "* [Part 5: Feature Selection](#Part-5:-Feature-Selection)\n",
    "* [Part 6: Model Selection](#Part-6:-Model-Selection)\n",
    "* [Part 7: Model Evaluation](#Part-7:-Model-Evaluation)\n",
    "* [Part 8: Summary](#Part-7:-Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Dataset\n",
    "\n",
    "The raw dataset are stored in data/ directory. The two-year data files are separated into 4 quarters for each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = r'./data/' \n",
    "all_files = glob.glob(path + \"/2016*.csv.gz\")\n",
    "\n",
    "list_2016 = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    list_2016.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'./data/' \n",
    "all_files = glob.glob(path + \"/2017*.csv.gz\")\n",
    "\n",
    "list_2017 = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    list_2017.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine same year data\n",
    "data_2016 = pd.concat(list_2016, axis=0)\n",
    "data_2017 = pd.concat(list_2016, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2016.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionans of dataset\n",
    "print ('2016 dataset has ' + str(data_2016.shape[0]) + ' rows and ' + str(data_2016.shape[1]) + ' columns')\n",
    "print ('2017 dataset has ' + str(data_2017.shape[0]) + ' rows and ' + str(data_2017.shape[1]) + ' columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two 2016, 2017 datasets\n",
    "data = pd.concat([data_2016, data_2017], axis=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique count for each feature\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.0: Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated values\n",
    "print ('duplicated rows =', data.duplicated().sum())\n",
    "print ('duplicated columns =', data.columns.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1: Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total missing values\n",
    "print ('missing value =', data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are lots of missing data, and we will remove features with no-entry first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing ratio\n",
    "missing_data = round(data.isnull().sum() / len(data), 2) * 100\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique missing ratios\n",
    "missing_data.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list missing_columns with missing ratio greater than 90%\n",
    "missing_columns = data.columns[missing_data > 90]\n",
    "print(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are ' + str(len(missing_columns)) + ' columns with missing ratio greater than 90%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with 90% missing data\n",
    "data = data.drop(missing_columns, axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = round(data.isnull().sum() / len(data), 2) * 100\n",
    "print(missing_data.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = data.columns[missing_data > 0]\n",
    "print('There are ' + str(len(missing_columns)) + ' columns with missing values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns descrition based on the data_dictionary: \n",
    "\n",
    "mths_since_last_record         (82%): The number of months since the last public record\n",
    "\n",
    "mths_since_recent_bc_dlq       (76%): Months since most recent bankcard delinquency\n",
    "\n",
    "mths_since_last_major_derog    (72%): Months since most recent 90-day or worse rating\n",
    "\n",
    "mths_since_recent_revol_delinq (65%): Months since most recent revolving delinquency\n",
    "\n",
    "mths_since_last_delinq         (49%): The number of months since the borrower's last delinquency\n",
    "\n",
    "next_pymnt_d                   (39%): Next scheduled payment date\n",
    "\n",
    "il_util                        (14%): Ratio of total current balance to high credit/credit limit on all install acct\n",
    "\n",
    "mths_since_recent_inq          (11%): Months since most recent inquiry\n",
    "\n",
    "emp_title                      ( 7%): The job title supplied by the Borrower when applying for the loan\n",
    "\n",
    "emp_length                     ( 7%): Employment length in years. \n",
    "\n",
    "num_tl_120dpd_2m               ( 5%): Number of accounts currently 120 days past due\n",
    "\n",
    "mths_since_rcnt_il             ( 3%): Months since most recent installment accounts opened\n",
    "\n",
    "mo_sin_old_il_acct             ( 3%): Months since oldest bank installment account opened\n",
    "\n",
    "title                          ( 3%): The loan title provided by the borrower\n",
    "\n",
    "mths_since_recent_bc           ( 1%): Months since most recent bankcard account opened\n",
    "\n",
    "bc_util                        ( 1%): Ratio of total current balance to high credit/credit limit for all bankcard accounts\n",
    "\n",
    "bc_open_to_buy                 ( 1%): Total open to buy on revolving bankcards\n",
    "\n",
    "percent_bc_gt_75               ( 1%): Percentage of all bankcard accounts > 75% of limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__These 18 columns with missing values are worth disccussing whether they are related to the scope of this project.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with 10% missing data\n",
    "data = data.drop(data.columns[missing_data > 10], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have dropped columns with more than 10% missing values. \n",
    "\n",
    "Now we take a close look at the data with missing values less than 10%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 emp_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_title statistics\n",
    "data['emp_title'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at first five entries of data['emp_title']\n",
    "data['emp_title'].values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value in emp_title with 'missing'\n",
    "data['emp_title'] = data['emp_title'].fillna(value='missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 emp_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_length statistics\n",
    "data['emp_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at unique entries of data['emp_length']\n",
    "data['emp_length'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emp_length'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value in emp_length with mode\n",
    "emp_length_mode = data['emp_length'].mode()[0]\n",
    "data['emp_length'] = data['emp_length'].fillna(value=emp_length_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 num_tl_120dpd_2m   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at first five entries of data['num_tl_120dpd_2m']\n",
    "data['num_tl_120dpd_2m'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values in num_tl_120dpd_2m\n",
    "data['num_tl_120dpd_2m'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tl_120dpd_2m statistics\n",
    "data['num_tl_120dpd_2m'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value in num_tl_120dpd_2m with 0\n",
    "data['num_tl_120dpd_2m'] = data['num_tl_120dpd_2m'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 mths_since_rcnt_il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at first ten entries of data['mths_since_rcnt_il']\n",
    "data['mths_since_rcnt_il'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mths_since_rcnt_il statistics\n",
    "data['mths_since_rcnt_il'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value with the median in mths_since_recent_inq\n",
    "mths_since_recent_inq_median = data['mths_since_rcnt_il'].median()\n",
    "data['mths_since_rcnt_il'] = data['mths_since_rcnt_il'].fillna(value=mths_since_recent_inq_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5 mo_sin_old_il_acct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mo_sin_old_il_acct statistics\n",
    "data['mo_sin_old_il_acct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value with the median in mo_sin_old_il_acct\n",
    "mo_sin_old_il_acct_median = data['mo_sin_old_il_acct'].median()\n",
    "data['mo_sin_old_il_acct'] = data['mo_sin_old_il_acct'].fillna(value=mo_sin_old_il_acct_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.6 title   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title statistics\n",
    "data['title'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values in title\n",
    "data['title'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value in title with 'missing'\n",
    "data['title'] = data['title'].fillna(value='missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.7 mths_since_recent_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mths_since_recent_bc statistics\n",
    "data['mths_since_recent_bc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value with the median in mths_since_recent_bc\n",
    "mths_since_recent_bc_median = data['mths_since_recent_bc'].median()\n",
    "data['mths_since_recent_bc'] = data['mths_since_recent_bc'].fillna(value=mths_since_recent_bc_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.8 bc_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc_util statistics\n",
    "data['bc_util'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value with the median in bc_util\n",
    "bc_util_median = data['bc_util'].median()\n",
    "data['bc_util'] = data['bc_util'].fillna(value=bc_util_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.9 bc_open_to_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc_open_to_buy statistics\n",
    "data['bc_open_to_buy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value with the median in percent_bc_gt_75\n",
    "bc_open_to_buy_median = data['bc_open_to_buy'].median()\n",
    "data['bc_open_to_buy'] = data['bc_open_to_buy'].fillna(value=bc_open_to_buy_median)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.10 percent_bc_gt_75  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent_bc_gt_75 statistics\n",
    "data['percent_bc_gt_75'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value with the median in percent_bc_gt_75\n",
    "percent_bc_gt_75_median = data['percent_bc_gt_75'].median()\n",
    "data['percent_bc_gt_75'] = data['percent_bc_gt_75'].fillna(value=percent_bc_gt_75_median)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total missing values\n",
    "print ('missing value =', data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total missing values\n",
    "print ('missing value =', data.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing ratio\n",
    "missing_data = round(data.isnull().sum() / len(data), 10) * 100\n",
    "print(missing_data.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value with the median in dti \n",
    "dti_median = data['dti'].median()\n",
    "data['dti'] = data['dti'].fillna(value=dti_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have cleaned missing data in the dataset. \n",
    "\n",
    "Next, we will take a look at the data types and its format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2: Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['object']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['term'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['term'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the heading and trailing whitespaces\n",
    "data['term'] = data['term'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode term into numerical variable\n",
    "term_map = {'36 months': 36, '60 months': 60} \n",
    "data['term'] = data['term'].apply(lambda x: term_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['term'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 emp_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emp_title'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_title has to many inputs, we can delete this column.\n",
    "data = data.drop('emp_title', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 emp_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emp_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode emp_length into numerical variable\n",
    "emp_length_map = {'< 1 year': 0, \n",
    "                  '1 year': 1, \n",
    "                  '2 years': 2, \n",
    "                  '3 years': 3, \n",
    "                  '4 years': 4, \n",
    "                  '5 years': 5, \n",
    "                  '6 years': 6, \n",
    "                  '7 years': 7, \n",
    "                  '8 years': 9, \n",
    "                  '9 years': 9, \n",
    "                  '10+ years': 10}\n",
    "data['emp_length'] = data['emp_length'].apply(lambda x: emp_length_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emp_length'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 home_ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['home_ownership'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['home_ownership'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine ANY and NONE type into Other\n",
    "data.loc[data['home_ownership'].isin(['ANY', 'NONE']), 'home_ownership'] = 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5 int_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert int_rate to float\n",
    "data['int_rate'] = data['int_rate'].apply(lambda x: pd.to_numeric(x.split(\"%\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['float'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable, which we want to compare across the independent variables, is loan status. \n",
    "\n",
    "The variables in the dataset that will affect the loan status are grade, sub_grade, annual income, purpose of the loan, loan_amount, annual income, emp_length etc.\n",
    "\n",
    "Here, we want to compare the average default rates across various independent variables and have a general idea about the variables that affect default rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 loan status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the target variable - loan_status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loan_status\n",
    "loan_status = data.groupby('loan_status').size()\n",
    "print(loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = loan_status.plot.pie(y='loan_status',figsize=(11, 11),autopct='%1.1f%%',legend=True,fontsize=12)\n",
    "plt.title('Loan Status')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are interested to know whether the loan is worth to invest or not, i.e., we want to know if the given loan is \"good\" or \"bad\". If the loan is \"fully paid off\", then it is good; it is bad if it is in the category of \"charged off\", \"default\", \"late (16-30 days)\" or \"late (31-120 days)\". For those that are in the \"current\" or \"in grace period\", we consider them as uncategoried due to uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['Current','In Grace Period']\n",
    "data = data[~data.loan_status.isin(category)]\n",
    "data['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode loan_status as 0 - good, 1 - bad\n",
    "data['loan_status'] = data['loan_status'].apply(lambda x: 0 if x=='Fully Paid' else 1)\n",
    "\n",
    "# convert loan_status to int\n",
    "data['loan_status'] = data['loan_status'].apply(lambda x: pd.to_numeric(x))\n",
    "\n",
    "data['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 loan grade and subgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loan grade\n",
    "loan_grade = data.groupby('grade').size()\n",
    "print(loan_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.countplot(x='grade', data=data, order=['A', 'B', 'C', 'D', 'E', 'F', 'G'])\n",
    "plt.xlabel(\"Grade\", labelpad=14)\n",
    "plt.ylabel(\"Count\", labelpad=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot default rates across grade of the loan\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.barplot(x='grade', y='loan_status', data=data, order=['A', 'B', 'C', 'D', 'E', 'F', 'G'])\n",
    "plt.xlabel(\"Grade\", labelpad=14)\n",
    "plt.ylabel(\"loan_status\", labelpad=14)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above barplot, the default rate is higher as loan grade moves from grade A to grade G. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.barplot(x='sub_grade', y='loan_status', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.groupby([\"sub_grade\"])['loan_status'].aggregate(np.sum).reset_index().sort_values('sub_grade')\n",
    "sns.barplot(x='sub_grade', y='loan_status', data=data order=result['sub_grade']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 loan title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change into lower case\n",
    "data['title'] = data['title'].apply(str.lower)\n",
    "lists = ['debt consolidation', 'credit card refinancing', 'business', 'vacation', \n",
    "         'home improvement', 'major purchase', 'medical expenses', 'car financing', \n",
    "         'moving and relocation', 'home buying', 'green loan', 'consolidation']\n",
    "\n",
    "data.loc[~data['title'].isin(lists), 'title'] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'].value_counts().plot(kind='barh', figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot default rates across loan title \n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "chart = sns.barplot(x='title', y='loan_status', data=data)\n",
    "plt.xlabel(\"title\", labelpad=14)\n",
    "plt.ylabel(\"loan_status\", labelpad=14)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=30)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 home_ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot default rates across home_ownership\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "chart = sns.barplot(x='home_ownership', y='loan_status', data=data)\n",
    "plt.xlabel(\"home_ownership\", labelpad=14)\n",
    "plt.ylabel(\"loan_status\", labelpad=14)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The renting has the highest default rate among four home-onwership categories. We also notice that the variance of other category is highest since it inculdes all other types of home owership.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 emp_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emp_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emp_length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot default rates across emp_length\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "chart = sns.barplot(x='emp_length', y='loan_status', data=data)\n",
    "ax.set(ylim=(0.24, 0.31))\n",
    "plt.xlabel(\"emp_length\", labelpad=14)\n",
    "plt.ylabel(\"loan_status\", labelpad=14)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dafault rate is between 0.27 to 0.30 for different emplyment lengths. The clients with longer employment length has lower default rate comparing with shorter employment length, but the difference is not that big.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this project is to predict whether a loan will be defaulted or not. To start with, let's identify the independent variables that are reated to the output - loan status. \n",
    "\n",
    "There are three types of independent variables that are closedly related to the defaultd loan. They are \n",
    "\n",
    "1. loan information, such as interest rate, loan amount, installment etc. \n",
    "\n",
    "2. application information, such as credit score, salary, occupation, age, assetts etc. \n",
    "\n",
    "3. customer behaviors which are generated after the loan is approved, such as delinquent amount, past-due amount. \n",
    "\n",
    "Notice that as an investor, we are not able to obtain type 3 - customer behaviors information before the loan is being approved. Hence, it cannot be included into the model prediction and we need to drop these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 unrelated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop variables that are related future customer behaviors\n",
    "behavior_var = [\n",
    "    'collection_recovery_fee',\n",
    "    'delinq_2yrs',\n",
    "    'delinq_amnt',\n",
    "    'earliest_cr_line',\n",
    "    'initial_list_status',\n",
    "    'inq_fi',\n",
    "    'inq_last_12m',\n",
    "    'inq_last_6mths',\n",
    "    'last_pymnt_amnt',\n",
    "    'last_pymnt_d',\n",
    "    'open_acc',\n",
    "    'open_acc_6m',\n",
    "    'out_prncp',\n",
    "    'out_prncp_inv',\n",
    "    'pub_rec',\n",
    "    'pymnt_plan',\n",
    "    'recoveries',\n",
    "    'revol_bal',\n",
    "    'revol_util',\n",
    "    'total_acc',\n",
    "    'total_pymnt',\n",
    "    'total_pymnt_inv',\n",
    "    'total_rec_prncp',\n",
    "    'total_rec_int',\n",
    "    'total_rec_late_fee',\n",
    "    'total_rec_prncp',\n",
    "    'total_rev_hi_lim',\n",
    "    'recoveries',\n",
    "    'last_pymnt_d',\n",
    "    'last_pymnt_amnt',\n",
    "    'last_credit_pull_d',\n",
    "    'application_type',\n",
    "    'disbursement_method',\n",
    "    'debt_settlement_flag']\n",
    "\n",
    "data = data.drop(behavior_var, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are about ' + str(data.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also drop some unrelated variables, such as Unnamed: 0, id, url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unrelated variables\n",
    "data = data.drop(['Unnamed: 0', 'id', 'url', 'policy_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are about ' + str(data.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 highly-correlated numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_data = data.select_dtypes(exclude=['object'])\n",
    "target = data['loan_status']\n",
    "num_data = num_data.drop('loan_status', axis=1)\n",
    "num_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the pair-wise correlation for numerical features\n",
    "correlation = num_data.corr()      \n",
    "\n",
    "# visualize the pair-wise correlation             \n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "sns.heatmap(correlation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the actual values of correlations\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the correlation between features  \n",
    "# remove one of two features that have a correlation >= 0.9\n",
    "\n",
    "columns = np.full((correlation.shape[0],), True, dtype=bool)\n",
    "for i in range(correlation.shape[0]):\n",
    "    for j in range(i + 1, correlation.shape[0]):\n",
    "        if correlation.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = num_data.columns[columns]\n",
    "num_data = num_data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 identify feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use feature imporatence feature in random forest algorithm. Note that columns with missing values cannot be put into the tree classifer, and hence we remove these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = num_data.dropna(axis='columns')\n",
    "print(num_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extc = ExtraTreesClassifier(random_state=10)\n",
    "extc = extc.fit(num_data, target)\n",
    "importances = extc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features that are not important\n",
    "features_to_drop = (importances <= 0.01)\n",
    "features_indexes = np.where(features_to_drop == True)\n",
    "print(features_indexes, importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select num_features that with important > 0.01\n",
    "select = (importances > 0.01)\n",
    "num_data = num_data.iloc[:, select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features\n",
    "cat_data = data.select_dtypes(include=['object']).drop(['sub_grade', 'issue_d', 'zip_code', 'addr_state'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label encode the selected categorical feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_encode = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grade\n",
    "cat_data['grade'] = lb_encode.fit_transform(cat_data['grade'])\n",
    "#cat_data['grade'].value_counts()\n",
    "\n",
    "# home_ownership\n",
    "cat_data['home_ownership'] = lb_encode.fit_transform(cat_data['home_ownership'])\n",
    "#cat_data['home_ownership'].value_counts()\n",
    "\n",
    "# verification_status\n",
    "cat_data['verification_status'] = lb_encode.fit_transform(cat_data['verification_status'])\n",
    "#cat_data['verification_status'].value_counts()\n",
    "\n",
    "cat_data['purpose'] = lb_encode.fit_transform(cat_data['purpose'])\n",
    "#cat_data['purpose'].value_counts()\n",
    "\n",
    "cat_data['title'] = lb_encode.fit_transform(cat_data['title'])\n",
    "#cat_data['title'].value_counts()\n",
    "\n",
    "cat_data['hardship_flag'] = lb_encode.fit_transform(cat_data['hardship_flag'])\n",
    "#cat_data['hardship_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data['hardship_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 selected feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([num_data, cat_data], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 splitting dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splite data into training and testing\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Reserve 20% for testing\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle = False)\n",
    "\n",
    "print('training data has ' + str(x_train.shape[0]) + ' observation with ' + str(x_train.shape[1]) + ' features')\n",
    "print('test data has ' + str(x_test.shape[0]) + ' observation with ' + str(x_test.shape[1]) + ' features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the propotion of y = 1\n",
    "print(y.sum() / y.shape * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dataset is imbalanced. Before we train the model, we need to resample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 random downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add y_train to the X_train to get a new df\n",
    "x_train['loan_status'] = y_train\n",
    "df = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling: balance the labels so we have the same number of default loan as undefault.\n",
    "default_number = len(df[df['loan_status'] == 1])\n",
    "print(\"Number of default\", default_number)\n",
    "\n",
    "default = (df[df['loan_status'] == 1])\n",
    "\n",
    "undefault = df[df['loan_status'] == 0].sample(n = default_number)\n",
    "print(\"Number of undefault\", len(undefault))\n",
    "\n",
    "df2 = default.append(undefault)\n",
    "print (df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train\n",
    "Y_train = df2['loan_status']\n",
    "\n",
    "# X_train\n",
    "to_drop = ['loan_status']\n",
    "X_train = df2.drop(to_drop, axis=1)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 scale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data, using standardization\n",
    "# standardization (x-mean)/std\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title build models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression\n",
    "classifier_logistic = LogisticRegression()\n",
    "\n",
    "# Random Forest\n",
    "classifier_RF = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 5-fold Cross Validation to get the accuracy for different models\n",
    "model_names = ['Logistic Regression', 'Random Forest']\n",
    "model_list = [classifier_logistic, classifier_RF]\n",
    "count = 0\n",
    "\n",
    "for classifier in model_list:\n",
    "    cv_score = model_selection.cross_val_score(classifier, X_train, Y_train, cv=5)\n",
    "    print(cv_score)\n",
    "    print('Model accuracy of ' + model_names[count] + ' is ' + str(cv_score.mean()))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1 Use Grid Search to Find Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# helper function for printing out grid search results \n",
    "def print_grid_search_metrics(gs):\n",
    "    print (\"Best score: \" + str(gs.best_score_))\n",
    "    print (\"Best parameters set:\")\n",
    "    best_parameters = gs.best_params_\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(param_name + ':' + str(best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Optimal Hyperparameters - LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible hyperparamter options for Logistic Regression Regularization\n",
    "# Penalty is choosed from L1 or L2\n",
    "# C is the lambda value(weight) for L1 and L2\n",
    "\n",
    "parameters = {\n",
    "    'penalty':('l1', 'l2'), \n",
    "    'C':(1, 5, 10)\n",
    "}\n",
    "Grid_LR = GridSearchCV(LogisticRegression(),parameters, cv=5)\n",
    "Grid_LR.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best hyperparameter combination\n",
    "print_grid_search_metrics(Grid_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "best_LR_model = Grid_LR.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Optimal Hyperparameters - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible hyperparamter options for Random Forest\n",
    "# Choose the number of trees\n",
    "parameters = {\n",
    "    'n_estimators' : [5,10,15]\n",
    "}\n",
    "Grid_RF = GridSearchCV(RandomForestClassifier(),parameters, cv=5)\n",
    "Grid_RF.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best number of tress\n",
    "print_grid_search_metrics(Grid_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best random forest\n",
    "best_RF_model = Grid_RF.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# calculate accuracy, precision and recall, [[tn, fp],[]]\n",
    "def cal_evaluation(classifier, cm):\n",
    "    tn = cm[0][0]\n",
    "    fp = cm[0][1]\n",
    "    fn = cm[1][0]\n",
    "    tp = cm[1][1]\n",
    "    accuracy  = (tp + tn) / (tp + fp + fn + tn + 0.0)\n",
    "    precision = tp / (tp + fp + 0.0)\n",
    "    recall = tp / (tp + fn + 0.0)\n",
    "    print (classifier)\n",
    "    print (\"Accuracy is: \" + str(accuracy))\n",
    "    print (\"precision is: \" + str(precision))\n",
    "    print (\"recall is: \" + str(recall))\n",
    "\n",
    "# print out confusion matrices\n",
    "def draw_confusion_matrices(confusion_matricies):\n",
    "    class_names = ['Not','Churn']\n",
    "    for cm in confusion_matrices:\n",
    "        classifier, cm = cm[0], cm[1]\n",
    "        cal_evaluation(classifier, cm)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Reds'))\n",
    "        plt.title('Confusion matrix for ' + classifier)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticklabels([''] + class_names)\n",
    "        ax.set_yticklabels([''] + class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix, accuracy, precison and recall for random forest and logistic regression\n",
    "confusion_matrices = [\n",
    "    (\"Logistic Regression\", confusion_matrix(y_test,best_LR_model.predict(x_test))),\n",
    "    (\"Random Forest\", confusion_matrix(y_test,best_RF_model.predict(x_test)))\n",
    "]\n",
    "\n",
    "draw_confusion_matrices(confusion_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build a model that informs investor which loans they should invest. The Lending Club loan data consists in 8 files collected quartely in year of 2016 and 2017. There are 118647 rows and 152 columns in the combined dataset. \n",
    "\n",
    "The target variable is loan_status, and we categorized the 'loan_status' in terms of final prediction values: default or non-default. \n",
    "\n",
    "Since there are more than 100 features, we reduce the number of features by doing the following \n",
    "\n",
    "1. drop the features with more than 10% missing data;\n",
    "\n",
    "2. drop the customer behavior features that are unknown before the loan is issued; \n",
    "\n",
    "3. remove highly correlated features.\n",
    "\n",
    "Then we used the feature importance from tree classifier to select top 10 numberical features, and we also manually selected 6 categorical features. \n",
    "\n",
    "We implemented logistic regression and random forest models to classify the loan_status with approximately 90% accuracy. \n",
    "\n",
    "There are still several places that we can improve this project. \n",
    "\n",
    "1. Here we only considered 16 features. It is definitey worth to try to put more feature to see whether it can imporve the model performance. \n",
    "\n",
    "2. We built two predicted models. There are lots of other binary classification models, such as gradient boosting and neural network models. \n",
    "\n",
    "3. We used down-sampling to resample the training set. There are several other resampling methods, for exmaple, over-sampling and Synthetic Minority Oversampling Technique (SMOTE). And it is worth to compare the these results with the dataset without any resampling. \n",
    "\n",
    "4. Here, the dataset consists two-year (2016 and 2017) data values. It is also a good idea to add data in most recent years and modify the models based on the new dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
